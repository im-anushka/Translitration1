{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Reduced dataset saved: kan_train_reduced.csv\n",
      "✅ Reduced dataset saved: kan_valid_reduced.csv\n",
      "✅ Reduced dataset saved: kan_test_reduced.csv\n",
      "Train Data: 3000 entries\n",
      "Validation Data: 600 entries\n",
      "Test Data: 300 entries\n",
      "\n",
      "Sample Train Data:\n",
      "        Kannada         English\n",
      "0    ಆಶ್ರಿತೇಷು      ashriteshu\n",
      "1  ಪ್ರಸ್ತಭೂಮಿಯ  prastabhoomiya\n",
      "2      ಮನೆವಾಡ್         manewad\n",
      "3       ವಿಹ್ವಲ         vihwala\n",
      "4    ಭಾವನೆಯಲ್ಲ    bhavaneyalla\n",
      "\n",
      "Sample Validation Data:\n",
      "          Kannada            English\n",
      "0  ಭಾವಗೀತೆಗಳನ್ನು  bhavagithegalannu\n",
      "1           ಸದೃಶ           sadrusha\n",
      "2     ಸೆಂಟಮೀಟರ್ನ       centameterna\n",
      "3      ನಾವೆಲ್ಲರೂ          navellaru\n",
      "4   ಸಮ್ಮೇಳನವನ್ನು     sammelanawannu\n",
      "\n",
      "Sample Test Data:\n",
      "               Kannada               English\n",
      "0               ಸಮದರ್              samaddar\n",
      "1  ಸಾವರಿಸಿಕೊಳ್ಳುತ್ತಲೇ  saavarisikolluttalae\n",
      "2            ತಿರಸ್ಕಾರ            tiraskaara\n",
      "3          ಇತಿಹಾಸತಜ್ಞ         itihaasatajna\n",
      "4          ಸೆರ್ವಿಲ್ಲೆ            sayreville\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load JSON lines from a file\n",
    "def load_json_lines(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))  # Convert each line to a dictionary\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to extract Kannada and English word pairs\n",
    "def extract_pairs(json_data):\n",
    "    kannada_words = [entry[\"native word\"] for entry in json_data]\n",
    "    english_words = [entry[\"english word\"] for entry in json_data]\n",
    "    return kannada_words, english_words\n",
    "\n",
    "# Function to load, clean, and sample JSON data as CSV\n",
    "def load_clean_and_sample_json(file_path, output_csv, sample_size):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                data.append({\"Kannada\": obj[\"native word\"], \"English\": obj[\"english word\"]})\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Reduce dataset size\n",
    "    df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    # Save cleaned and reduced dataset\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Reduced dataset saved: {output_csv}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define dataset paths\n",
    "train_json = r\"/home/gwl/Desktop/test/kan_train.json\"\n",
    "valid_json = r\"/home/gwl/Desktop/test/kan_valid.json\"\n",
    "test_json = r\"/home/gwl/Desktop/test/kan_test.json\"\n",
    "\n",
    "# Load, clean, and sample datasets\n",
    "train_df = load_clean_and_sample_json(train_json, \"kan_train_reduced.csv\", 3000)\n",
    "valid_df = load_clean_and_sample_json(valid_json, \"kan_valid_reduced.csv\", 600)\n",
    "test_df = load_clean_and_sample_json(test_json, \"kan_test_reduced.csv\", 300)\n",
    "\n",
    "# Load reduced datasets\n",
    "train_df = pd.read_csv(\"kan_train_reduced.csv\")\n",
    "valid_df = pd.read_csv(\"kan_valid_reduced.csv\")\n",
    "test_df = pd.read_csv(\"kan_test_reduced.csv\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train Data: {len(train_df)} entries\")\n",
    "print(f\"Validation Data: {len(valid_df)} entries\")\n",
    "print(f\"Test Data: {len(test_df)} entries\")\n",
    "\n",
    "# Print first 5 rows for verification\n",
    "print(\"\\nSample Train Data:\\n\", train_df.head())\n",
    "print(\"\\nSample Validation Data:\\n\", valid_df.head())\n",
    "print(\"\\nSample Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 3000 entries\n",
      "Validation Data: 600 entries\n",
      "Test Data: 300 entries\n",
      "\n",
      "Sample Training Data:\n",
      "        Kannada         English  \\\n",
      "0    ಆಶ್ರಿತೇಷು      ashriteshu   \n",
      "1  ಪ್ರಸ್ತಭೂಮಿಯ  prastabhoomiya   \n",
      "2      ಮನೆವಾಡ್         manewad   \n",
      "3       ವಿಹ್ವಲ         vihwala   \n",
      "4    ಭಾವನೆಯಲ್ಲ    bhavaneyalla   \n",
      "\n",
      "                                      Kannada_tokens  \\\n",
      "0  [277, 28083, 29001, 24793, 26372, 25795, 27088...   \n",
      "1  [277, 27796, 24793, 26372, 27673, 24793, 27088...   \n",
      "2  [277, 27034, 26466, 27090, 27320, 26657, 27425...   \n",
      "3  [277, 27320, 25795, 28116, 24793, 27320, 27062...   \n",
      "4  [277, 29170, 26657, 27320, 26466, 27090, 27449...   \n",
      "\n",
      "                       English_tokens  \n",
      "0          [5624, 3851, 1483, 491, 2]  \n",
      "1  [7152, 1842, 4692, 10526, 4908, 2]  \n",
      "2                 [320, 2952, 666, 2]  \n",
      "3               [1496, 6778, 8274, 2]  \n",
      "4       [12129, 6578, 3263, 10334, 2]  \n",
      "\n",
      "Sample Validation Data:\n",
      "          Kannada            English  \\\n",
      "0  ಭಾವಗೀತೆಗಳನ್ನು  bhavagithegalannu   \n",
      "1           ಸದೃಶ           sadrusha   \n",
      "2     ಸೆಂಟಮೀಟರ್ನ       centameterna   \n",
      "3      ನಾವೆಲ್ಲರೂ          navellaru   \n",
      "4   ಸಮ್ಮೇಳನವನ್ನು     sammelanawannu   \n",
      "\n",
      "                                      Kannada_tokens  \\\n",
      "0  [277, 29170, 26657, 27320, 27427, 28614, 27088...   \n",
      "1               [277, 27673, 26121, 30966, 29001, 2]   \n",
      "2  [277, 27673, 27090, 27375, 28426, 27034, 28614...   \n",
      "3  [277, 26466, 26657, 27320, 27090, 27062, 24793...   \n",
      "4  [277, 27673, 27034, 24793, 27034, 28106, 27821...   \n",
      "\n",
      "                                  English_tokens  \n",
      "0  [705, 16877, 1290, 24903, 8915, 4844, 491, 2]  \n",
      "1                         [2219, 10837, 1336, 2]  \n",
      "2                      [582, 441, 7488, 2159, 2]  \n",
      "3                         [21659, 24273, 491, 2]  \n",
      "4      [450, 4350, 725, 336, 2236, 4844, 491, 2]  \n",
      "\n",
      "Sample Test Data:\n",
      "               Kannada               English  \\\n",
      "0               ಸಮದರ್              samaddar   \n",
      "1  ಸಾವರಿಸಿಕೊಳ್ಳುತ್ತಲೇ  saavarisikolluttalae   \n",
      "2            ತಿರಸ್ಕಾರ            tiraskaara   \n",
      "3          ಇತಿಹಾಸತಜ್ಞ         itihaasatajna   \n",
      "4          ಸೆರ್ವಿಲ್ಲೆ            sayreville   \n",
      "\n",
      "                                      Kannada_tokens  \\\n",
      "0        [277, 27673, 27034, 26121, 26372, 24793, 2]   \n",
      "1  [277, 27673, 26657, 27320, 26372, 25795, 27673...   \n",
      "2  [277, 27088, 25795, 26372, 27673, 24793, 27093...   \n",
      "3  [277, 27937, 27088, 25795, 28116, 26657, 27673...   \n",
      "4  [277, 27673, 27090, 26372, 24793, 27320, 25795...   \n",
      "\n",
      "                                      English_tokens  \n",
      "0                              [11754, 7809, 365, 2]  \n",
      "1  [3595, 18787, 559, 1719, 6984, 15620, 1465, 43...  \n",
      "2                         [596, 972, 26987, 1561, 2]  \n",
      "3           [25, 305, 1336, 478, 533, 2374, 2159, 2]  \n",
      "4                               [254, 164, 11876, 2]  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load reduced dataset\n",
    "train_df = pd.read_csv(\"kan_train_reduced.csv\")\n",
    "valid_df = pd.read_csv(\"kan_valid_reduced.csv\")\n",
    "test_df = pd.read_csv(\"kan_test_reduced.csv\")\n",
    "\n",
    "# Use a pre-trained tokenizer (IndicTrans) with trust_remote_code=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Ensure all values are strings and handle NaNs\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df[\"Kannada\"] = df[\"Kannada\"].astype(str).fillna(\"\")\n",
    "    df[\"English\"] = df[\"English\"].astype(str).fillna(\"\")\n",
    "\n",
    "# Tokenize Kannada & English\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df[\"Kannada_tokens\"] = df[\"Kannada\"].apply(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"][0].tolist())\n",
    "    df[\"English_tokens\"] = df[\"English\"].apply(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"][0].tolist())\n",
    "\n",
    "# Save tokenized datasets\n",
    "train_df.to_csv(\"kan_train_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "valid_df.to_csv(\"kan_valid_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(\"kan_test_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training Data: {len(train_df)} entries\")\n",
    "print(f\"Validation Data: {len(valid_df)} entries\")\n",
    "print(f\"Test Data: {len(test_df)} entries\")\n",
    "\n",
    "# Print sample tokenized data\n",
    "print(\"\\nSample Training Data:\\n\", train_df.head())\n",
    "print(\"\\nSample Validation Data:\\n\", valid_df.head())\n",
    "print(\"\\nSample Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Kannada', 'English', 'Kannada_tokens', 'English_tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"kan_train_tokenized.csv\")\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset Class\n",
    "import ast\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Convert string representation of lists to actual lists if needed\n",
    "        self.kannada = dataframe[\"Kannada_tokens\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "        self.english = dataframe[\"English_tokens\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kannada)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.kannada[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.english[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Define collate function (Move outside class)\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad sequences using tokenizer.pad_token_id\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\"input_ids\": input_ids_padded, \"labels\": labels_padded}\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = TransliterationDataset(train_df)\n",
    "valid_dataset = TransliterationDataset(valid_df)\n",
    "test_dataset = TransliterationDataset(test_df)\n",
    "\n",
    "# Create DataLoader with correct collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load Pre-trained IndicTrans Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwl/anaconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/gwl/anaconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   aas | Actual:   ortiz\n",
      "Predicted:   chavavannu | Actual:   cheluvamba\n",
      "Predicted:   jagaligal | Actual:   jateyagi\n",
      "Predicted:   sasollu | Actual:   sangharshavu\n",
      "Predicted:   madagidda | Actual:   madalagiddu\n",
      "Epoch 1, Training Loss: 6.2469, Validation Loss: 2.7696, Test Accuracy: 0.33%, BLEU Score: 0.1724, Character Error Rate (CER): 0.4978\n",
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   a | Actual:   ortiz\n",
      "Predicted:   chalavana | Actual:   cheluvamba\n",
      "Predicted:   jateyagi | Actual:   jateyagi\n",
      "Predicted:   sasharavu | Actual:   sangharshavu\n",
      "Predicted:   matadalaagiddu | Actual:   madalagiddu\n",
      "Epoch 2, Training Loss: 2.4052, Validation Loss: 1.9587, Test Accuracy: 2.00%, BLEU Score: 0.3733, Character Error Rate (CER): 0.3419\n",
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   oerj | Actual:   ortiz\n",
      "Predicted:   chaluvemba | Actual:   cheluvamba\n",
      "Predicted:   jateyagi | Actual:   jateyagi\n",
      "Predicted:   sambaryavu | Actual:   sangharshavu\n",
      "Predicted:   madalagiddu | Actual:   madalagiddu\n",
      "Epoch 3, Training Loss: 1.6019, Validation Loss: 1.5105, Test Accuracy: 9.33%, BLEU Score: 0.5110, Character Error Rate (CER): 0.2487\n",
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   aart | Actual:   ortiz\n",
      "Predicted:   chelluva | Actual:   cheluvamba\n",
      "Predicted:   jateyagi | Actual:   jateyagi\n",
      "Predicted:   sambarshavu | Actual:   sangharshavu\n",
      "Predicted:   madalagiddu | Actual:   madalagiddu\n",
      "Epoch 4, Training Loss: 1.1342, Validation Loss: 1.3295, Test Accuracy: 12.33%, BLEU Score: 0.5823, Character Error Rate (CER): 0.2161\n",
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   aartj | Actual:   ortiz\n",
      "Predicted:   chelluva | Actual:   cheluvamba\n",
      "Predicted:   jateyagi | Actual:   jateyagi\n",
      "Predicted:   samarshavu | Actual:   sangharshavu\n",
      "Predicted:   maadalaagiddu | Actual:   madalagiddu\n",
      "Epoch 5, Training Loss: 0.7880, Validation Loss: 1.2145, Test Accuracy: 15.33%, BLEU Score: 0.6388, Character Error Rate (CER): 0.1945\n",
      "\n",
      "🔍 Sample Predictions:\n",
      "Predicted:   arth | Actual:   ortiz\n",
      "Predicted:   chelluvantba | Actual:   cheluvamba\n",
      "Predicted:   jateyagi | Actual:   jateyagi\n",
      "Predicted:   sambarshavu | Actual:   sangharshavu\n",
      "Predicted:   maadalaagiddu | Actual:   madalagiddu\n",
      "Epoch 6, Training Loss: 0.5364, Validation Loss: 1.1773, Test Accuracy: 17.33%, BLEU Score: 0.6363, Character Error Rate (CER): 0.1877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training Loop with Validation Loss, Test Accuracy, BLEU Score, and CER\n",
    "def train_model(model, train_loader, valid_loader, test_loader, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Create attention mask\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate test accuracy, BLEU Score, and CER\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_bleu = 0\n",
    "        total_cer = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "                outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=50)\n",
    "\n",
    "                predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                actual_texts = [tokenizer.decode(label[label != tokenizer.pad_token_id], skip_special_tokens=True) for label in labels]\n",
    "\n",
    "                # Compute exact match accuracy\n",
    "                correct += sum(1 for p, a in zip(predicted_texts, actual_texts) if p.strip().lower() == a.strip().lower())\n",
    "                total += len(actual_texts)\n",
    "\n",
    "                # Compute BLEU Score and CER\n",
    "                for p, a in zip(predicted_texts, actual_texts):\n",
    "                    reference = [list(a)]  # BLEU expects a list of references\n",
    "                    candidate = list(p)  # Convert prediction into a list of characters\n",
    "                    total_bleu += sentence_bleu(reference, candidate)\n",
    "\n",
    "                    total_cer += levenshtein_distance(p, a) / max(len(a), 1)  # Normalize by actual text length\n",
    "\n",
    "        test_accuracy = (correct / total) * 100\n",
    "        avg_bleu = total_bleu / total\n",
    "        avg_cer = total_cer / total\n",
    "\n",
    "        # Print sample predictions for all epochs\n",
    "        print(\"\\n🔍 Sample Predictions:\")\n",
    "        for p, a in zip(predicted_texts[:5], actual_texts[:5]):\n",
    "            print(f\"Predicted: {p} | Actual: {a}\")\n",
    "\n",
    "        # Print metrics for each epoch\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss/len(valid_loader):.4f}, \"\n",
    "              f\"Test Accuracy: {test_accuracy:.2f}%, \"\n",
    "              f\"BLEU Score: {avg_bleu:.4f}, \"\n",
    "              f\"Character Error Rate (CER): {avg_cer:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, valid_loader, test_loader, epochs=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully at kan_transliteration_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the save path\n",
    "model_save_path = \"kan_transliteration_model.pth\"\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"✅ Model saved successfully at {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17639/3077134934.py\", line 2, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1965, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1964, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1976, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 38, in <module>\n",
      "    from .auto_factory import _LazyAutoMapping\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1964, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1976, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py\", line 30, in <module>\n",
      "    from transformers.generation.candidate_generator import AssistantVocabTranslatorCache\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/generation/candidate_generator.py\", line 27, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_17639/3077134934.py\", line 2, in <module>\n",
      "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1965, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1964, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1976, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py\", line 38, in <module>\n",
      "    from .auto_factory import _LazyAutoMapping\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 40, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1964, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/utils/import_utils.py\", line 1976, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py\", line 30, in <module>\n",
      "    from transformers.generation.candidate_generator import AssistantVocabTranslatorCache\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/transformers/generation/candidate_generator.py\", line 27, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 15, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py\", line 17, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/core/api.py\", line 1, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/gwl/anaconda3/lib/python3.12/site-packages/pandas/_libs/__init__.py\", line 17, in <module>\n",
      "    import pandas._libs.pandas_datetime  # noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(\"kan_transliteration_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_kannada_to_english(kannada_text):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Tokenize input Kannada text\n",
    "    input_tokens = tokenizer(kannada_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "    # Decode generated tokens to English text\n",
    "    english_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return english_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kannada: ನಮಸ್ಕಾರ -> English:   namkaara\n",
      "Kannada: ಶಿಕ್ಷಣ -> English:   shikhana\n",
      "Kannada: ಪ್ರೀತಿ -> English:   priati\n",
      "Kannada: ನಂಬಿಕೆ -> English:   nambike\n",
      "Kannada: ಪದ್ಯ -> English:   padya\n"
     ]
    }
   ],
   "source": [
    "sample_kannada_words = [\"ನಮಸ್ಕಾರ\", \"ಶಿಕ್ಷಣ\", \"ಪ್ರೀತಿ\", \"ನಂಬಿಕೆ\", \"ಪದ್ಯ\"]\n",
    "\n",
    "for word in sample_kannada_words:\n",
    "    transliterated_word = transliterate_kannada_to_english(word)\n",
    "    print(f\"Kannada: {word} -> English: {transliterated_word}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
