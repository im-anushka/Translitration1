{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reduced dataset saved: kan_train_reduced.csv\n",
      "‚úÖ Reduced dataset saved: kan_valid_reduced.csv\n",
      "‚úÖ Reduced dataset saved: kan_test_reduced.csv\n",
      "Train Data: 3000 entries\n",
      "Validation Data: 600 entries\n",
      "Test Data: 300 entries\n",
      "\n",
      "Sample Train Data:\n",
      "           English      Kannada\n",
      "0      ashriteshu    ‡≤Ü‡≤∂‡≥ç‡≤∞‡≤ø‡≤§‡≥á‡≤∑‡≥Å\n",
      "1  prastabhoomiya  ‡≤™‡≥ç‡≤∞‡≤∏‡≥ç‡≤§‡≤≠‡≥Ç‡≤Æ‡≤ø‡≤Ø\n",
      "2         manewad      ‡≤Æ‡≤®‡≥Ü‡≤µ‡≤æ‡≤°‡≥ç\n",
      "3         vihwala       ‡≤µ‡≤ø‡≤π‡≥ç‡≤µ‡≤≤\n",
      "4    bhavaneyalla    ‡≤≠‡≤æ‡≤µ‡≤®‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤\n",
      "\n",
      "Sample Validation Data:\n",
      "              English        Kannada\n",
      "0  bhavagithegalannu  ‡≤≠‡≤æ‡≤µ‡≤ó‡≥Ä‡≤§‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å\n",
      "1           sadrusha           ‡≤∏‡≤¶‡≥É‡≤∂\n",
      "2       centameterna     ‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤Æ‡≥Ä‡≤ü‡≤∞‡≥ç‡≤®\n",
      "3          navellaru      ‡≤®‡≤æ‡≤µ‡≥Ü‡≤≤‡≥ç‡≤≤‡≤∞‡≥Ç\n",
      "4     sammelanawannu   ‡≤∏‡≤Æ‡≥ç‡≤Æ‡≥á‡≤≥‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å\n",
      "\n",
      "Sample Test Data:\n",
      "                 English             Kannada\n",
      "0              samaddar               ‡≤∏‡≤Æ‡≤¶‡≤∞‡≥ç\n",
      "1  saavarisikolluttalae  ‡≤∏‡≤æ‡≤µ‡≤∞‡≤ø‡≤∏‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤§‡≥ç‡≤§‡≤≤‡≥á\n",
      "2            tiraskaara            ‡≤§‡≤ø‡≤∞‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞\n",
      "3         itihaasatajna          ‡≤á‡≤§‡≤ø‡≤π‡≤æ‡≤∏‡≤§‡≤ú‡≥ç‡≤û\n",
      "4            sayreville          ‡≤∏‡≥Ü‡≤∞‡≥ç‡≤µ‡≤ø‡≤≤‡≥ç‡≤≤‡≥Ü\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to load JSON lines from a file\n",
    "def load_json_lines(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data.append(json.loads(line.strip()))  # Convert each line to a dictionary\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "# Function to extract Kannada and English word pairs\n",
    "def extract_pairs(json_data):\n",
    "    kannada_words = [entry[\"native word\"] for entry in json_data]\n",
    "    english_words = [entry[\"english word\"] for entry in json_data]\n",
    "    return kannada_words, english_words\n",
    "\n",
    "# Function to load, clean, and sample JSON data as CSV\n",
    "def load_clean_and_sample_json(file_path, output_csv, sample_size):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                obj = json.loads(line.strip())\n",
    "                data.append({\"English\": obj[\"english word\"], \"Kannada\": obj[\"native word\"]})\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid line in {file_path}: {e}\")\n",
    "    \n",
    "    # Convert to Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Reduce dataset size\n",
    "    df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    # Save cleaned and reduced dataset\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Reduced dataset saved: {output_csv}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Define dataset paths\n",
    "train_json = r\"/home/gwl/Desktop/test/kan_train.json\"\n",
    "valid_json = r\"/home/gwl/Desktop/test/kan_valid.json\"\n",
    "test_json = r\"/home/gwl/Desktop/test/kan_test.json\"\n",
    "\n",
    "# Load, clean, and sample datasets\n",
    "train_df = load_clean_and_sample_json(train_json, \"kan_train_reduced.csv\", 3000)\n",
    "valid_df = load_clean_and_sample_json(valid_json, \"kan_valid_reduced.csv\", 600)\n",
    "test_df = load_clean_and_sample_json(test_json, \"kan_test_reduced.csv\", 300)\n",
    "\n",
    "# Load reduced datasets\n",
    "train_df = pd.read_csv(\"kan_train_reduced.csv\")\n",
    "valid_df = pd.read_csv(\"kan_valid_reduced.csv\")\n",
    "test_df = pd.read_csv(\"kan_test_reduced.csv\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train Data: {len(train_df)} entries\")\n",
    "print(f\"Validation Data: {len(valid_df)} entries\")\n",
    "print(f\"Test Data: {len(test_df)} entries\")\n",
    "\n",
    "# Print first 5 rows for verification\n",
    "print(\"\\nSample Train Data:\\n\", train_df.head())\n",
    "print(\"\\nSample Validation Data:\\n\", valid_df.head())\n",
    "print(\"\\nSample Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 3000 entries\n",
      "Validation Data: 600 entries\n",
      "Test Data: 300 entries\n",
      "\n",
      "Sample Training Data:\n",
      "           English      Kannada                      English_tokens  \\\n",
      "0      ashriteshu    ‡≤Ü‡≤∂‡≥ç‡≤∞‡≤ø‡≤§‡≥á‡≤∑‡≥Å          [5624, 3851, 1483, 491, 2]   \n",
      "1  prastabhoomiya  ‡≤™‡≥ç‡≤∞‡≤∏‡≥ç‡≤§‡≤≠‡≥Ç‡≤Æ‡≤ø‡≤Ø  [7152, 1842, 4692, 10526, 4908, 2]   \n",
      "2         manewad      ‡≤Æ‡≤®‡≥Ü‡≤µ‡≤æ‡≤°‡≥ç                 [320, 2952, 666, 2]   \n",
      "3         vihwala       ‡≤µ‡≤ø‡≤π‡≥ç‡≤µ‡≤≤               [1496, 6778, 8274, 2]   \n",
      "4    bhavaneyalla    ‡≤≠‡≤æ‡≤µ‡≤®‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤       [12129, 6578, 3263, 10334, 2]   \n",
      "\n",
      "                                      Kannada_tokens  \n",
      "0  [277, 28083, 29001, 24793, 26372, 25795, 27088...  \n",
      "1  [277, 27796, 24793, 26372, 27673, 24793, 27088...  \n",
      "2  [277, 27034, 26466, 27090, 27320, 26657, 27425...  \n",
      "3  [277, 27320, 25795, 28116, 24793, 27320, 27062...  \n",
      "4  [277, 29170, 26657, 27320, 26466, 27090, 27449...  \n",
      "\n",
      "Sample Validation Data:\n",
      "              English        Kannada  \\\n",
      "0  bhavagithegalannu  ‡≤≠‡≤æ‡≤µ‡≤ó‡≥Ä‡≤§‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å   \n",
      "1           sadrusha           ‡≤∏‡≤¶‡≥É‡≤∂   \n",
      "2       centameterna     ‡≤∏‡≥Ü‡≤Ç‡≤ü‡≤Æ‡≥Ä‡≤ü‡≤∞‡≥ç‡≤®   \n",
      "3          navellaru      ‡≤®‡≤æ‡≤µ‡≥Ü‡≤≤‡≥ç‡≤≤‡≤∞‡≥Ç   \n",
      "4     sammelanawannu   ‡≤∏‡≤Æ‡≥ç‡≤Æ‡≥á‡≤≥‡≤®‡≤µ‡≤®‡≥ç‡≤®‡≥Å   \n",
      "\n",
      "                                  English_tokens  \\\n",
      "0  [705, 16877, 1290, 24903, 8915, 4844, 491, 2]   \n",
      "1                         [2219, 10837, 1336, 2]   \n",
      "2                      [582, 441, 7488, 2159, 2]   \n",
      "3                         [21659, 24273, 491, 2]   \n",
      "4      [450, 4350, 725, 336, 2236, 4844, 491, 2]   \n",
      "\n",
      "                                      Kannada_tokens  \n",
      "0  [277, 29170, 26657, 27320, 27427, 28614, 27088...  \n",
      "1               [277, 27673, 26121, 30966, 29001, 2]  \n",
      "2  [277, 27673, 27090, 27375, 28426, 27034, 28614...  \n",
      "3  [277, 26466, 26657, 27320, 27090, 27062, 24793...  \n",
      "4  [277, 27673, 27034, 24793, 27034, 28106, 27821...  \n",
      "\n",
      "Sample Test Data:\n",
      "                 English             Kannada  \\\n",
      "0              samaddar               ‡≤∏‡≤Æ‡≤¶‡≤∞‡≥ç   \n",
      "1  saavarisikolluttalae  ‡≤∏‡≤æ‡≤µ‡≤∞‡≤ø‡≤∏‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤§‡≥ç‡≤§‡≤≤‡≥á   \n",
      "2            tiraskaara            ‡≤§‡≤ø‡≤∞‡≤∏‡≥ç‡≤ï‡≤æ‡≤∞   \n",
      "3         itihaasatajna          ‡≤á‡≤§‡≤ø‡≤π‡≤æ‡≤∏‡≤§‡≤ú‡≥ç‡≤û   \n",
      "4            sayreville          ‡≤∏‡≥Ü‡≤∞‡≥ç‡≤µ‡≤ø‡≤≤‡≥ç‡≤≤‡≥Ü   \n",
      "\n",
      "                                      English_tokens  \\\n",
      "0                              [11754, 7809, 365, 2]   \n",
      "1  [3595, 18787, 559, 1719, 6984, 15620, 1465, 43...   \n",
      "2                         [596, 972, 26987, 1561, 2]   \n",
      "3           [25, 305, 1336, 478, 533, 2374, 2159, 2]   \n",
      "4                               [254, 164, 11876, 2]   \n",
      "\n",
      "                                      Kannada_tokens  \n",
      "0        [277, 27673, 27034, 26121, 26372, 24793, 2]  \n",
      "1  [277, 27673, 26657, 27320, 26372, 25795, 27673...  \n",
      "2  [277, 27088, 25795, 26372, 27673, 24793, 27093...  \n",
      "3  [277, 27937, 27088, 25795, 28116, 26657, 27673...  \n",
      "4  [277, 27673, 27090, 26372, 24793, 27320, 25795...  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load reduced dataset\n",
    "train_df = pd.read_csv(\"kan_train_reduced.csv\")\n",
    "valid_df = pd.read_csv(\"kan_valid_reduced.csv\")\n",
    "test_df = pd.read_csv(\"kan_test_reduced.csv\")\n",
    "\n",
    "# Use a pre-trained tokenizer (IndicTrans) with trust_remote_code=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Ensure all values are strings and handle NaNs\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df[\"Kannada\"] = df[\"Kannada\"].astype(str).fillna(\"\")\n",
    "    df[\"English\"] = df[\"English\"].astype(str).fillna(\"\")\n",
    "\n",
    "# Tokenize Kannada & English\n",
    "for df in [train_df, valid_df, test_df]:\n",
    "    df[\"English_tokens\"] = df[\"English\"].apply(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"][0].tolist())\n",
    "    df[\"Kannada_tokens\"] = df[\"Kannada\"].apply(lambda x: tokenizer(x, return_tensors=\"pt\")[\"input_ids\"][0].tolist())\n",
    "\n",
    "# Save tokenized datasets\n",
    "train_df.to_csv(\"kan_train_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "valid_df.to_csv(\"kan_valid_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "test_df.to_csv(\"kan_test_tokenized.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training Data: {len(train_df)} entries\")\n",
    "print(f\"Validation Data: {len(valid_df)} entries\")\n",
    "print(f\"Test Data: {len(test_df)} entries\")\n",
    "\n",
    "# Print sample tokenized data\n",
    "print(\"\\nSample Training Data:\\n\", train_df.head())\n",
    "print(\"\\nSample Validation Data:\\n\", valid_df.head())\n",
    "print(\"\\nSample Test Data:\\n\", test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset Class\n",
    "import ast\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        # Convert string representation of lists to actual lists if needed\n",
    "        self.english = dataframe[\"English_tokens\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "        self.kannada = dataframe[\"Kannada_tokens\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.kannada)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(self.english[idx], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(self.kannada[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Define collate function (Move outside class)\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad sequences using tokenizer.pad_token_id\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return {\"input_ids\": input_ids_padded, \"labels\": labels_padded}\n",
    "\n",
    "# Create Dataset\n",
    "train_dataset = TransliterationDataset(train_df)\n",
    "valid_dataset = TransliterationDataset(valid_df)\n",
    "test_dataset = TransliterationDataset(test_df)\n",
    "\n",
    "# Create DataLoader with correct collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load Pre-trained IndicTrans Model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gwl/anaconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/gwl/anaconda3/lib/python3.12/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥Ü‡≤ü‡≥ç‡≤ü‡≤ø‡≤∞‡≤ø‡≤§‡≤ø‡≤ó‡≤ø | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤ï‡≥ç‡≤®‡≤ø‡≤∏‡≥ç‡≤ü‡≥ç‡≤ü‡≥ç‡≤ü‡≥Å‡≤µ‡≥Å | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥Ü‡≤Ø‡≥ç‡≤®‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤∏‡≥ç‡≤ü‡≥ç‡≤ü‡≥ç‡≤ü‡≤ø‡≤∏‡≤∞‡≥ç‡≤µ‡≥Å‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≤ø‡≤∏‡≥ç‡≤ü‡≥ç‡≤ü‡≤ø‡≤ó‡≤≥‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 1, Training Loss: 4.2090, Validation Loss: 1.0365, Test Accuracy: 0.00%, BLEU Score: 0.1138, Character Error Rate (CER): 0.9357\n",
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≥Å‡≤Ç‡≤ü‡≥ç‡≤∞‡≤ø‡≤∏‡≥ç | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≥Ü‡≤®‡≥ç‡≤®‡≥Ü‡≤≤‡≥Å‡≤µ‡≤Ç‡≤¨ | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≥Å‡≤Ç‡≤ü‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≥ç‡≤®‡≤Ç‡≤ó‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≤ø‡≤¶‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 2, Training Loss: 0.6379, Validation Loss: 0.3065, Test Accuracy: 0.00%, BLEU Score: 0.2586, Character Error Rate (CER): 0.8386\n",
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≥ç‡≤®‡≤æ‡≤∞‡≥ç‡≤§‡≤ø‡≤ú‡≥ç | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤®‡≥Ü‡≤≤‡≥ç‡≤≤‡≥Å‡≤µ‡≤Ç‡≤¨ | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤®‡≤æ‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤®‡≤Ç‡≤ò‡≥ç‡≤∑‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤®‡≤æ‡≤°‡≥ç‡≤Ø‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 3, Training Loss: 0.2261, Validation Loss: 0.2259, Test Accuracy: 0.00%, BLEU Score: 0.2826, Character Error Rate (CER): 0.8525\n",
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥ç‡≤∞‡≥à‡≤®‡≥ç‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤Ç‡≤¨ | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥á‡≤≥‡≥ç‡≤£‡≤æ‡≤Ç‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ö‡≥ç‡≤®‡≤æ‡≤Ç‡≤ò‡≤∂‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≥‡≥ç‡≤®‡≤ø‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 4, Training Loss: 0.1400, Validation Loss: 0.2043, Test Accuracy: 0.33%, BLEU Score: 0.2887, Character Error Rate (CER): 0.8513\n",
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≤ø‡≤®‡≥ç‡≤®‡≥ä‡≤∞‡≥ç‡≤§‡≤ø‡≤ú‡≥ç | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥Å‡≤®‡≥ç‡≤®‡≥Ü‡≤≤‡≥Å‡≤µ‡≤Ç‡≤¨ | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥Å‡≤®‡≥ç‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≥Å‡≤®‡≥ç‡≤®‡≤æ‡≤Ç‡≤ò‡≤∂‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≥Ü‡≤≤‡≥ç‡≤ü‡≤ø‡≤®‡≥ç‡≤®‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 5, Training Loss: 0.0974, Validation Loss: 0.2086, Test Accuracy: 0.00%, BLEU Score: 0.2632, Character Error Rate (CER): 0.9337\n",
      "\n",
      "üîç Sample Predictions:\n",
      "Predicted:   ‡≤π‡≥á‡≤ü‡≥ç‡≤∞‡≤ø‡≤£‡≥ç‡≤£‡≤æ‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç | Actual:   ‡≤í‡≤∞‡≥ç‡≤ü‡≤ø‡≤ú‡≥ç\n",
      "Predicted:   ‡≤π‡≥á‡≤ü‡≥ç‡≤∞‡≤ø‡≤ï‡≥ç‡≤®‡≥Ü‡≤≤‡≥Å‡≤µ‡≤Ç‡≤¨ | Actual:   ‡≤ö‡≥Ü‡≤≤‡≥Å‡≤µ‡≤æ‡≤Ç‡≤¨\n",
      "Predicted:   ‡≤π‡≥á‡≤≤‡≤ø forget‡≤®‡≥ç‡≤®‡≤æ‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø | Actual:   ‡≤ú‡≤§‡≥Ü‡≤Ø‡≤æ‡≤ó‡≤ø\n",
      "Predicted:   ‡≤π‡≥á‡≤≤‡≤ø‡≤ü‡≥ç‡≤∞‡≤∂‡≥ç‡≤µ‡≥Å | Actual:   ‡≤∏‡≤Ç‡≤ò‡≤∞‡≥ç‡≤∑‡≤µ‡≥Å\n",
      "Predicted:   ‡≤π‡≥á‡≤≥‡≥ç‡≤®‡≤ø‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å | Actual:   ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Å\n",
      "Epoch 6, Training Loss: 0.0734, Validation Loss: 0.2174, Test Accuracy: 0.33%, BLEU Score: 0.2686, Character Error Rate (CER): 0.9211\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, tokenizer, train_loader, valid_loader, test_loader, epochs=3):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Create attention mask\n",
    "            attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calculate test accuracy, BLEU Score, and CER\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_bleu = 0\n",
    "        total_cer = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                attention_mask = (input_ids != tokenizer.pad_token_id).to(device)\n",
    "\n",
    "                # Convert input tokens to text\n",
    "                input_texts = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "                # Add language prefix `<2kn>` for Kannada\n",
    "                input_texts = [f\"<2kn> {text}\" for text in input_texts]\n",
    "\n",
    "                # Tokenize again with prefix\n",
    "                inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "                # Generate Kannada predictions\n",
    "                outputs = model.generate(**inputs, max_length=50)\n",
    "\n",
    "                predicted_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                actual_texts = [\n",
    "                    tokenizer.decode(label[label != tokenizer.pad_token_id], skip_special_tokens=True)\n",
    "                    for label in labels\n",
    "                ]\n",
    "\n",
    "                # Compute accuracy, BLEU, and CER\n",
    "                correct += sum(1 for p, a in zip(predicted_texts, actual_texts) if p.strip().lower() == a.strip().lower())\n",
    "                total += len(actual_texts)\n",
    "\n",
    "                for p, a in zip(predicted_texts, actual_texts):\n",
    "                    reference = [list(a)]  # BLEU expects a list of references\n",
    "                    candidate = list(p)  # Convert prediction into a list of characters\n",
    "                    total_bleu += sentence_bleu(reference, candidate)\n",
    "                    total_cer += levenshtein_distance(p, a) / max(len(a), 1)  # Normalize by actual text length\n",
    "\n",
    "        test_accuracy = (correct / total) * 100\n",
    "        avg_bleu = total_bleu / total\n",
    "        avg_cer = total_cer / total\n",
    "\n",
    "        # Print sample predictions for all epochs\n",
    "        print(\"\\nüîç Sample Predictions:\")\n",
    "        for p, a in zip(predicted_texts[:5], actual_texts[:5]):\n",
    "            print(f\"Predicted: {p} | Actual: {a}\")\n",
    "\n",
    "        # Print metrics for each epoch\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss/len(valid_loader):.4f}, \"\n",
    "              f\"Test Accuracy: {test_accuracy:.2f}%, \"\n",
    "              f\"BLEU Score: {avg_bleu:.4f}, \"\n",
    "              f\"Character Error Rate (CER): {avg_cer:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, tokenizer, train_loader, valid_loader, test_loader, epochs=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved successfully at engtokan_transliteration_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the save path\n",
    "model_save_path = \"engtokan_transliteration_model.pth\"\n",
    "\n",
    "# Save the model state dictionary\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully at {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-en-indic-1B\", trust_remote_code=True)\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(\"engtokan_transliteration_model.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "# Move model to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transliterate_english_to_kannada(english_text):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    # Tokenize input English text\n",
    "    input_tokens = tokenizer(english_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate prediction\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**input_tokens, max_length=50)\n",
    "\n",
    "    # Decode generated tokens to Kannada text\n",
    "    kannada_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    return kannada_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Namaskara -> Kannada:   ‡≤®‡≤æ‡≤Æ‡≤∏‡≥ç‡≤ï‡≤∞\n",
      "English: Shikshana -> Kannada:   ‡≤∂‡≥Ä‡≤ï‡≥ç‡≤∑‡≤®‡≤æ\n",
      "English: Preeti -> Kannada:   ‡≤™‡≥ç‡≤∞‡≥Ä‡≤§‡≤ø\n",
      "English: Nambike -> Kannada:   ‡≤®‡≤Ç‡≤¨‡≤ø‡≤ï‡≥Ü\n",
      "English: Padya -> Kannada:   ‡≤™‡≤æ‡≤¶‡≥ç‡≤Ø‡≤æ\n"
     ]
    }
   ],
   "source": [
    "sample_english_words = [\"Namaskara\", \"Shikshana\", \"Preeti\", \"Nambike\", \"Padya\"]\n",
    "\n",
    "for word in sample_english_words:\n",
    "    transliterated_word = transliterate_english_to_kannada(word)\n",
    "    print(f\"English: {word} -> Kannada: {transliterated_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
